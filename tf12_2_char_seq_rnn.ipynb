{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "tf12-2- char-seq-rnn.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNKkzIX4hAqYSVlHze7uAVQ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/lala991204/DL-self-study/blob/master/tf12_2_char_seq_rnn.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "이전의 코드와는 다르게 문자열만 입력해주면 알아서 해주는 코드"
      ],
      "metadata": {
        "id": "NuN-yW0RuhcR"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T6YBG7CRuFtU",
        "outputId": "83fc3776-1b0c-41cd-f616-789a004929c2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tf.Tensor(\n",
            "[[[0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
            "  [0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
            "  [0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
            "  [0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]\n",
            "  [0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
            "  [0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
            "  [0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
            "  [0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
            "  [1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "  [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "  [0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
            "  [0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
            "  [0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]\n",
            "  [0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]]], shape=(1, 14, 10), dtype=float32)\n",
            "(1, 14, 10)\n",
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " lstm_1 (LSTM)               (None, 14, 10)            840       \n",
            "                                                                 \n",
            " time_distributed_1 (TimeDis  (None, 14, 10)           110       \n",
            " tributed)                                                       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 950\n",
            "Trainable params: 950\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/50\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super(Adam, self).__init__(name, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 2s 2s/step - loss: 2.2939 - accuracy: 0.1429\n",
            "Epoch 2/50\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 2.1291 - accuracy: 0.3571\n",
            "Epoch 3/50\n",
            "1/1 [==============================] - 0s 21ms/step - loss: 1.9374 - accuracy: 0.3571\n",
            "Epoch 4/50\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 1.7403 - accuracy: 0.5000\n",
            "Epoch 5/50\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 1.4879 - accuracy: 0.7143\n",
            "Epoch 6/50\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 1.2068 - accuracy: 0.6429\n",
            "Epoch 7/50\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 0.9002 - accuracy: 0.7857\n",
            "Epoch 8/50\n",
            "1/1 [==============================] - 0s 15ms/step - loss: 0.6519 - accuracy: 0.8571\n",
            "Epoch 9/50\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 0.4722 - accuracy: 0.9286\n",
            "Epoch 10/50\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 0.3264 - accuracy: 0.9286\n",
            "Epoch 11/50\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 0.2242 - accuracy: 1.0000\n",
            "Epoch 12/50\n",
            "1/1 [==============================] - 0s 16ms/step - loss: 0.1557 - accuracy: 1.0000\n",
            "Epoch 13/50\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 0.1078 - accuracy: 1.0000\n",
            "Epoch 14/50\n",
            "1/1 [==============================] - 0s 15ms/step - loss: 0.0732 - accuracy: 1.0000\n",
            "Epoch 15/50\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 0.0490 - accuracy: 1.0000\n",
            "Epoch 16/50\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 0.0330 - accuracy: 1.0000\n",
            "Epoch 17/50\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 0.0228 - accuracy: 1.0000\n",
            "Epoch 18/50\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 0.0164 - accuracy: 1.0000\n",
            "Epoch 19/50\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 0.0122 - accuracy: 1.0000\n",
            "Epoch 20/50\n",
            "1/1 [==============================] - 0s 15ms/step - loss: 0.0092 - accuracy: 1.0000\n",
            "Epoch 21/50\n",
            "1/1 [==============================] - 0s 16ms/step - loss: 0.0071 - accuracy: 1.0000\n",
            "Epoch 22/50\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 0.0056 - accuracy: 1.0000\n",
            "Epoch 23/50\n",
            "1/1 [==============================] - 0s 20ms/step - loss: 0.0045 - accuracy: 1.0000\n",
            "Epoch 24/50\n",
            "1/1 [==============================] - 0s 15ms/step - loss: 0.0036 - accuracy: 1.0000\n",
            "Epoch 25/50\n",
            "1/1 [==============================] - 0s 15ms/step - loss: 0.0030 - accuracy: 1.0000\n",
            "Epoch 26/50\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 0.0025 - accuracy: 1.0000\n",
            "Epoch 27/50\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 0.0022 - accuracy: 1.0000\n",
            "Epoch 28/50\n",
            "1/1 [==============================] - 0s 15ms/step - loss: 0.0019 - accuracy: 1.0000\n",
            "Epoch 29/50\n",
            "1/1 [==============================] - 0s 20ms/step - loss: 0.0016 - accuracy: 1.0000\n",
            "Epoch 30/50\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 0.0014 - accuracy: 1.0000\n",
            "Epoch 31/50\n",
            "1/1 [==============================] - 0s 21ms/step - loss: 0.0013 - accuracy: 1.0000\n",
            "Epoch 32/50\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 0.0011 - accuracy: 1.0000\n",
            "Epoch 33/50\n",
            "1/1 [==============================] - 0s 19ms/step - loss: 0.0010 - accuracy: 1.0000\n",
            "Epoch 34/50\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 9.2587e-04 - accuracy: 1.0000\n",
            "Epoch 35/50\n",
            "1/1 [==============================] - 0s 19ms/step - loss: 8.4774e-04 - accuracy: 1.0000\n",
            "Epoch 36/50\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 7.8142e-04 - accuracy: 1.0000\n",
            "Epoch 37/50\n",
            "1/1 [==============================] - 0s 19ms/step - loss: 7.2474e-04 - accuracy: 1.0000\n",
            "Epoch 38/50\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 6.7602e-04 - accuracy: 1.0000\n",
            "Epoch 39/50\n",
            "1/1 [==============================] - 0s 22ms/step - loss: 6.3381e-04 - accuracy: 1.0000\n",
            "Epoch 40/50\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 5.9704e-04 - accuracy: 1.0000\n",
            "Epoch 41/50\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 5.6481e-04 - accuracy: 1.0000\n",
            "Epoch 42/50\n",
            "1/1 [==============================] - 0s 16ms/step - loss: 5.3641e-04 - accuracy: 1.0000\n",
            "Epoch 43/50\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 5.1123e-04 - accuracy: 1.0000\n",
            "Epoch 44/50\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 4.8881e-04 - accuracy: 1.0000\n",
            "Epoch 45/50\n",
            "1/1 [==============================] - 0s 15ms/step - loss: 4.6876e-04 - accuracy: 1.0000\n",
            "Epoch 46/50\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 4.5074e-04 - accuracy: 1.0000\n",
            "Epoch 47/50\n",
            "1/1 [==============================] - 0s 16ms/step - loss: 4.3448e-04 - accuracy: 1.0000\n",
            "Epoch 48/50\n",
            "1/1 [==============================] - 0s 21ms/step - loss: 4.1979e-04 - accuracy: 1.0000\n",
            "Epoch 49/50\n",
            "1/1 [==============================] - 0s 24ms/step - loss: 4.0644e-04 - accuracy: 1.0000\n",
            "Epoch 50/50\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 3.9429e-04 - accuracy: 1.0000\n",
            "[[[8.77048478e-06 5.98349561e-06 2.53805865e-05 4.05498613e-05\n",
            "   1.11716718e-05 4.07505977e-05 7.20161097e-06 9.99403954e-01\n",
            "   4.56220791e-04 1.14304598e-07]\n",
            "  [7.99978068e-07 4.72152294e-07 1.70524430e-07 2.35706210e-07\n",
            "   4.13377155e-09 9.99761164e-01 4.19719953e-07 2.33113184e-04\n",
            "   3.07411096e-06 5.10954862e-07]\n",
            "  [1.97626704e-07 6.05833702e-06 4.30880914e-10 4.28413671e-07\n",
            "   2.85407484e-08 4.83941812e-06 1.26722283e-04 2.65896670e-04\n",
            "   9.99595702e-01 1.11158144e-07]\n",
            "  [1.05335057e-04 1.25191343e-06 6.19523416e-05 9.99588072e-01\n",
            "   5.55091560e-07 1.66919875e-07 5.63261665e-07 1.11795445e-04\n",
            "   4.87520483e-05 8.15804087e-05]\n",
            "  [6.59151624e-07 1.12888820e-05 9.99823511e-01 7.33232469e-07\n",
            "   9.49511445e-07 4.56818816e-05 1.66398386e-05 7.40543110e-05\n",
            "   2.39052028e-10 2.65379731e-05]\n",
            "  [1.15045430e-06 3.19134506e-06 6.90299930e-05 9.47551460e-09\n",
            "   6.26831076e-09 9.99474943e-01 3.84128973e-04 4.66592319e-05\n",
            "   3.58367828e-07 2.04652915e-05]\n",
            "  [1.21781864e-06 1.33735398e-06 1.06160473e-06 6.43754987e-08\n",
            "   5.87295936e-08 2.78871041e-04 9.99305844e-01 4.82016185e-05\n",
            "   3.52920615e-04 1.03807506e-05]\n",
            "  [9.99666452e-01 1.36987597e-04 1.86877867e-06 6.08078517e-06\n",
            "   9.95281880e-07 2.22270751e-06 1.33569745e-04 3.12568591e-05\n",
            "   5.68371524e-06 1.48945519e-05]\n",
            "  [1.33315596e-04 9.99834180e-01 3.27907378e-06 3.87251475e-09\n",
            "   8.92175844e-09 2.16973422e-06 6.49096273e-06 2.87850327e-07\n",
            "   2.01184521e-05 1.84250212e-07]\n",
            "  [5.26318991e-06 2.38188449e-07 4.33460838e-04 2.98204054e-06\n",
            "   1.07161092e-07 3.01712862e-04 6.78445867e-05 1.72620173e-06\n",
            "   3.41862773e-07 9.99186337e-01]\n",
            "  [3.40198749e-05 9.45191118e-07 3.72836809e-07 2.63604352e-06\n",
            "   2.43840645e-08 9.99876976e-01 2.35936623e-05 3.36695775e-05\n",
            "   1.34346567e-06 2.63244983e-05]\n",
            "  [6.11865403e-07 1.01575915e-05 1.97844893e-10 3.48699530e-07\n",
            "   1.50311212e-08 5.45191824e-07 2.54858605e-04 2.77212202e-05\n",
            "   9.99704897e-01 7.78751257e-07]\n",
            "  [2.15427557e-04 3.66448967e-06 7.21976030e-05 9.99569476e-01\n",
            "   5.58637169e-07 1.80096464e-07 4.60465117e-07 5.62617715e-05\n",
            "   1.85521367e-05 6.32802767e-05]\n",
            "  [6.03274827e-07 1.03052262e-05 9.99844790e-01 7.00090709e-07\n",
            "   8.57311875e-07 3.59166370e-05 1.50799715e-05 6.45816690e-05\n",
            "   2.14470150e-10 2.70831333e-05]]]\n",
            "\tPrediction str:  f you want you\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "\n",
        "sample = \"if you want you\"\n",
        "idx2char = list(set(sample))     # index -> char\n",
        "char2idx = {c: i for i, c in enumerate(idx2char)}    # char -> idex(각각의 알파벳의 인덱스를 부여)\n",
        "\n",
        "# hyper parameters\n",
        "dic_size = len(char2idx)         # RNN input size (one hot size)\n",
        "hidden_size = len(char2idx)      # RNN output size\n",
        "num_classes = len(char2idx)      # final output size (RNN or softmax, etc.)\n",
        "batch_size = 1       # one sample data, one batch (1개의 문자열)\n",
        "sequence_length = len(sample) - 1   # number of lstm rollings (unit # -> cell의 개수)\n",
        "learning_rate = 0.1\n",
        "\n",
        "sample_idx = [char2idx[c] for c in sample]      # char to index(해당 문자열 순서에 맞게 인덱스 부여)\n",
        "x_data = [sample_idx[:-1]]        # X_data sample (0 ~ n-1) hello: hell\n",
        "y_data = [sample_idx[1:]]         # Y label sample (1 ~ n)  hello: ello\n",
        "\n",
        "x_one_hot_eager = tf.one_hot(x_data, num_classes)\n",
        "x_one_hot_numpy = tf.keras.utils.to_categorical(x_data, num_classes)\n",
        "y_one_hot_eager = tf.one_hot(y_data, num_classes)\n",
        "print(x_one_hot_eager)\n",
        "print(x_one_hot_eager.shape)\n",
        "\n",
        "tf.model = tf.keras.Sequential()\n",
        "tf.model.add(tf.keras.layers.\n",
        "             LSTM(units=num_classes, input_shape=(sequence_length, x_one_hot_eager.shape[2]), return_sequences=True))\n",
        "tf.model.add(tf.keras.layers.TimeDistributed(tf.keras.layers.Dense(units=num_classes, activation='softmax')))\n",
        "tf.model.summary()\n",
        "tf.model.compile(loss='categorical_crossentropy', optimizer=tf.keras.optimizers.Adam(lr=learning_rate),\n",
        "                 metrics=['accuracy']) \n",
        "tf.model.fit(x_one_hot_eager, y_one_hot_eager, epochs=50)\n",
        "\n",
        "predictions = tf.model.predict(x_one_hot_eager)\n",
        "print(predictions)\n",
        "\n",
        "for i, prediction in enumerate(predictions):\n",
        "    # print char using argmax, dict\n",
        "    result_str = [idx2char[c] for c in np.argmax(prediction, axis=1)]\n",
        "    print(\"\\tPrediction str: \", ''.join(result_str))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "hklpvv5V3HEZ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}